{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to machine learning & the supercompressible metamaterial example (3D)\n",
    "\n",
    "M.A. Bessa (m.a.bessa@tudelft.nl)\\\n",
    "September 30, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What:** 30min introduction to Machine Learning\n",
    "\n",
    "**How:** Jointly workout this notebook\n",
    "* GitHub: https://github.com/bessagroup/f3dasm\n",
    "    1. You can do this locally in your computer (but you have to have the Python packages installed):\n",
    "        * clone the repository to your computer: git clone https://github.com/bessagroup/f3dasm\n",
    "        * load jupyter notebook (it will open in your internet browser): jupyter notebook\n",
    "        * search for this notebook in your computer (f3dasm/examples/supercompressible/machine_learning) and open it\n",
    "    2. Or you can use Google's Colab (no installation required, but times out if idle):\n",
    "        * go to https://colab.research.google.com\n",
    "        * login\n",
    "        * File > Open notebook\n",
    "        * click on Github (no need to login or authorize anything)\n",
    "        * paste the git link: https://github.com/bessagroup/f3dasm\n",
    "        * click search and then click on the notebook (f3dasm/examples/supercompressible/machine_learning).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple tutorial is based on a script I created for this article: https://imechanica.org/node/23957\n",
    "\n",
    "It follows from some examples provided by the scikit-learn user guide, which seem to have originated from Mathieu Blondel, Jake Vanderplas, Vincent Dubourg, and Jan Hendrik Metzen.\n",
    "\n",
    "License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's start by importing basic modules into Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline for today\n",
    "\n",
    "1. Basic steps in machine learning (supervised learning)\n",
    "2. Our first machine learning model: Gaussian Process regression\n",
    "3. Is machine learning necessary? Simple example with noisy dataset\n",
    "4. Replicating the classification plots of the supercompressible metamaterial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic steps in supervised learning\n",
    "\n",
    "Today we'll cover 4 basic steps in machine learning (supervised learning)\n",
    "\n",
    "1. Get to know your dataset\n",
    "2. Pre-process your data (split dataset into **training** and **testing** set)\n",
    "3. Choose your machine learning method and fit it to the training set.\n",
    "4. Predict the accuracy of the machine learning model on the testing set.\n",
    "\n",
    "We will start with a very simple example: regression in 1D\n",
    "\n",
    "### 1. First look at your dataset\n",
    "\n",
    "- Here we will create an \"artificial\" dataset with 50 points from the function $x\\sin(x)$ in the domain $x\\in[0,10]$.\n",
    "- In general, we wouldn't know the underlying function (of course!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # fundamental scientific computing module\n",
    "import matplotlib.pyplot as plt # plotting module\n",
    "import pandas as pd # Pandas dataframe (very common in ML to handle datasets)\n",
    "\n",
    "# Function to \"learn\"\n",
    "def f(x):\n",
    "    return x * np.sin(x)\n",
    "\n",
    "n_artificial_data = 50 # number of points in our dataset\n",
    "x_artificial_data = np.linspace(0, 10, n_artificial_data) # uniformly spaced points\n",
    "y_artificial_data = f(x_artificial_data) # function values at x_data\n",
    "\n",
    "#\n",
    "# Create a dictionary with the names of the input and output variables\n",
    "# and their respective values:\n",
    "input_dictionary = {\n",
    "            'x' : x_artificial_data,\n",
    "            'y' : y_artificial_data\n",
    "            }\n",
    "#\n",
    "# Then, we create a Pandas data frame:\n",
    "data = pd.DataFrame(input_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, \"data\" is our dataset (a Pandas dataframe), just like what F3DASM outputs when it finishes running.\n",
    "\n",
    "- Given a dataset, before doing anything else make sure you understand the data and its structure (inputs, outputs, names of variables, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset \"x\" are the inputs (features) and \"y\" are the outputs (targets).\n",
    "\n",
    "- Save input points (features) in a variable called X_data\n",
    "\n",
    "- Save output points (targets) in a variable called Y_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = data.loc[:,'x'].values # note that we ask for the\n",
    "                                # values, not a subset of the\n",
    "                                # DataFrame\n",
    "\n",
    "y_data = data.loc[:,'y'].values\n",
    "\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A short note:\n",
    "\n",
    "- In most machine learning packages the inputs and outputs should be 2D arrays (even if the variables are 1D vectors, like in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.reshape(x_data, (-1, 1)) # convert vector to 2d array (ML packages use input variables as 2d arrays)\n",
    "\n",
    "Y_data = np.reshape(y_data, (-1, 1)) # convert vector to 2d array (ML packages use input variables as 2d arrays)\n",
    "\n",
    "print(X_data)\n",
    "print(Y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pre-process your data\n",
    "\n",
    "Prior to any learning task, datasets should be pre-processed (scaling, cleaning, etc.).\n",
    "\n",
    "- Here, we will only do one thing: spliting the data into training and testing data.\n",
    "\n",
    "A powerful and very popular machine learning package for this is **scikit-learn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 1987 # set a random seed so that everyone gets the same result\n",
    "np.random.seed(seed)\n",
    "\n",
    "testset_ratio = 0.90 # ratio of test set points from the dataset\n",
    "\n",
    "# Let's split into 10% training points and the rest for testing:\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data,\n",
    "                                    Y_data, test_size=testset_ratio,\n",
    "                                    random_state=seed)\n",
    "\n",
    "#x_train = X_train.ravel() # just for plotting later\n",
    "#x_test = X_test.ravel() # just for plotting later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the **training** and **testing** points, as well as the underlying function that originated them (usually we don't know the underlying function!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots() # This opens a new figure\n",
    "\n",
    "# Plot points and interpolate them:\n",
    "ax1.plot(X_data, y_data, 'r:', markersize=6, linewidth=2,\n",
    "         label=u'ground truth: $f(x) = x\\,\\sin(x)$')\n",
    "\n",
    "ax1.plot(X_train, Y_train, 'k*', markersize=12,\n",
    "         label=\"Training points (data)\") # Markers locating training points\n",
    "\n",
    "ax1.plot(X_test, Y_test, 'ro', markersize=6,\n",
    "         label=\"Testing points (data)\") # Markers locating training points\n",
    "\n",
    "ax1.set_xlabel('$x$') # label of the x axis\n",
    "ax1.set_ylabel('$f(x)$') # label of the y axis\n",
    "ax1.legend(loc='upper left') # plot legend in the upper left corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes (a really simple!) dataset pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Choose machine learning model and fit to training set\n",
    "\n",
    "Suppose that our choice of \"machine learning\" model is just a simple \"polynomial approximation\"\n",
    "\n",
    "- Fitting a polynomial with scikit-learn is trivial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing the polynomial predictor from scikit-learn\n",
    "from sklearn.preprocessing import PolynomialFeatures # For Polynomial fit\n",
    "from sklearn.linear_model import LinearRegression # For Least Squares\n",
    "from sklearn.pipeline import make_pipeline # to link different objects\n",
    "\n",
    "degree = 4 # degree of polynomial we want to fit\n",
    "\n",
    "# Create polynomial model:\n",
    "poly_model = make_pipeline(PolynomialFeatures(degree),\n",
    "                           LinearRegression())\n",
    "\n",
    "# Fit polynomial model to your training data:\n",
    "poly_model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done.\n",
    "\n",
    "- We have a \"machine learning\" model that fits our training data.\n",
    "- Now, let's use our model to predict the testing points (**unseen** by the algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_pred = poly_model.predict(X_test) # prediction of our polynomial\n",
    "                                         # for unseen points (test data)\n",
    "\n",
    "print(Y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the polynomial prediction on top of fig1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot x_data and prediction as a blue line:\n",
    "ax1.plot(X_test, Y_test_pred, 'bo', linewidth=2,\n",
    "         label=\"Prediction at testing points\")\n",
    "\n",
    "ax1.plot(X_data, poly_model.predict(X_data), 'b:', linewidth=2,\n",
    "         label=\"Polynomial of degree %d prediction\" % degree)\n",
    "\n",
    "# Replot figure and legend:\n",
    "ax1.legend(loc='upper left')\n",
    "fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice.\n",
    "\n",
    "Yet, our polynomial (blue) is clearly different of the function we want to \"learn\", i.e. $x \\sin(x)$.\n",
    "\n",
    "How do we evaluate the quality of our approximation?\n",
    "\n",
    "### 4. Evaluate model performance\n",
    "\n",
    "* By evaluating the error of our polynomial model in the points that we didn't use in the fit (testing points).\n",
    "\n",
    "Two common metrics are $R^2$ and $MSE$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import error metrics:\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Compute MSE and R2 for the polynomial model we fitted\n",
    "mse_value_poly = mean_squared_error(Y_test, Y_test_pred)\n",
    "r2_value_poly = r2_score(Y_test, Y_test_pred)\n",
    "\n",
    "print('MSE for polynomial = ', mse_value_poly)\n",
    "print('R2 score for polynomial = ', r2_value_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, these predictions are not great because:\n",
    "\n",
    "* We want $MSE$ to be as low as possible\n",
    "\n",
    "* The closer $R^2$ is to 1.0 the better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial models can hardly be considered \"machine learning\"...\n",
    "\n",
    "Instead, let's use a machine learning method called Gaussian Processes -- a powerful method for small datasets (<10k points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, ExpSineSquared, ConstantKernel\n",
    "\n",
    "# Define the kernel function\n",
    "kernel = ConstantKernel(1.0, (1e-3, 1e3)) * RBF(10, (1e-2, 1e2)) # This is the standard RBF kernel\n",
    "#kernel = 1.0 * RBF(10, (1e-2, 1e2)) # Same kernel as above\n",
    "                                    #(scikit-learn assumes constant\n",
    "                                    # variance if you just write RBF\n",
    "                                    # without the constant kernel or\n",
    "                                    # without multiplying by 1.0)\n",
    "\n",
    "# Other examples of kernels:\n",
    "#kernel = ExpSineSquared(length_scale=3.0, periodicity=3.14,\n",
    "#                       length_scale_bounds=(0.1, 10.0),\n",
    "#                       periodicity_bounds=(0.1, 10)) * RBF(3.0, (1e-2, 1e2))\n",
    "#kernel = Matern(length_scale=1.0, length_scale_bounds=(1e-2, 1e2),nu=1.5)\n",
    "                \n",
    "gp_model = GaussianProcessRegressor(kernel=kernel, alpha=1e-10, n_restarts_optimizer=20) # using a small alpha\n",
    "\n",
    "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "gp_model.fit(X_train, Y_train)\n",
    "\n",
    "# Make the prediction on the entire dataset (for plotting)\n",
    "Y_data_pred, sigma_data_pred = gp_model.predict(X_data, return_std=True) # also output the uncertainty (standard deviation)\n",
    "\n",
    "SIGMA_data_pred = np.reshape(sigma_data_pred, (-1, 1)) # convert this 1D array into a 2D array for consistency\n",
    "\n",
    "# Predict for test set (for error metric)\n",
    "Y_test_pred, sigma_test_pred = gp_model.predict(X_test, return_std=True) # also output the uncertainty (standard deviation)\n",
    "\n",
    "SIGMA_test_pred = np.reshape(sigma_test_pred, (-1, 1)) # convert this 1D array into a 2D array for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the function, the prediction and the 95% confidence interval based on the MSE\n",
    "fig2, ax2 = plt.subplots() # This opens a new figure\n",
    "\n",
    "ax2.plot(X_data, Y_data, 'r:', label=u'ground truth: $f(x) = x\\,\\sin(x)$') # function to learn\n",
    "\n",
    "ax2.plot(X_train, Y_train, 'ro', markersize=6, label=\"training points\") # noiseless data\n",
    "\n",
    "ax2.plot(X_test, Y_test, 'kx', markersize=6, label=\"testing points\") # Plot test points\n",
    "\n",
    "ax2.plot(X_data, Y_data_pred, 'b-', label=\"GPR prediction\")\n",
    "ax2.fill(np.concatenate([X_data, X_data[::-1]]),\n",
    "         np.concatenate([Y_data_pred - 1.9600 * SIGMA_data_pred,\n",
    "                        (Y_data_pred + 1.9600 * SIGMA_data_pred)[::-1]]),\n",
    "         alpha=.5, fc='b', ec='None', label='95% confidence interval')\n",
    "ax2.set_xlabel('$x$')\n",
    "ax2.set_ylabel('$f(x)$')\n",
    "ax2.set_title(\"Posterior kernel: %s\"\n",
    "              % gp_model.kernel_, fontsize=12) # Show in the title the value of the hyperparameters\n",
    "ax2.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a much better prediction than what we obtained with the polynomial fit! In addition, also note that we predict the 95\\% confidence interval for our predictions!\n",
    "\n",
    "- Let's use the regression metrics $R^2$ and $MSE$ to compare the two models (polynomial regression vs GPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MSE and R2 for the GP model we just fitted:\n",
    "mse_value_gp = mean_squared_error(Y_test, Y_test_pred)\n",
    "r2_value_gp = r2_score(Y_test, Y_test_pred)\n",
    "\n",
    "print('MSE for GP = ', mse_value_gp)\n",
    "print('R2 score for GP = ', r2_value_gp)\n",
    "\n",
    "\n",
    "# Printing the previously obtained metrics for the polynomial fit:\n",
    "print('MSE for polynomial = ', mse_value_poly)\n",
    "print('R2 score for polynomial = ', r2_value_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian Process model is much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification plots for the supercompressible metamaterial (3D)\n",
    "\n",
    "The beautiful thing about modern machine learning packages, is that they are as easy to use for large dimensional datasets as they are for the 1D example that we just covered.\n",
    "\n",
    "- Let's consider the 3D dataset obtained by F3DASM for the supercompressible metamaterial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "# third-party\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "binary_metrics = ((accuracy_score, 'Accuracy score'),\n",
    "                  (f1_score, 'F1 score'),\n",
    "                  (precision_score, 'Precision score'),\n",
    "                  (recall_score, 'Recall score'))\n",
    "\n",
    "colors = ['r', 'g', 'b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import a dataset (previously obtained with F3DASM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pandas frame\n",
    "filename = 'DoE_results.pkl'\n",
    "with open(filename, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    \n",
    "# define number of points to start with\n",
    "N = 1000\n",
    "\n",
    "points = data['points'].loc[:(N-1)]\n",
    "print('variables:', [col for col in points.columns])\n",
    "\n",
    "# get number of inputs\n",
    "n_inputs = len(points.columns) - 3\n",
    "\n",
    "# get X data\n",
    "X_data = points.iloc[:,range(n_inputs)].values.copy()\n",
    "\n",
    "# get y data\n",
    "var_name = 'coilable'\n",
    "y_data = points.loc[:, var_name].values.copy()\n",
    "\n",
    "Y_data = np.reshape(y_data, (-1, 1)) # convert vector to 2d array (ML packages use input variables as 2d arrays)\n",
    "\n",
    "print('X:', X_data)\n",
    "print('X shape:', np.shape(X_data))\n",
    "print('Y:', Y_data)\n",
    "print('Y shape:', np.shape(Y_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is a classification problem, let's see how balanced the classes are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_ratio = 0.20 # ratio of test set points from the dataset\n",
    "\n",
    "# Let's split into 80% training points and the rest for testing:\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data,\n",
    "                                    Y_data, test_size=testset_ratio,\n",
    "                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data and fit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# train\n",
    "clf = SVC(kernel='rbf').fit(X_train_scaled, Y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model with accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(clf, train_data, test_data, metrics=((accuracy_score, 'Accuracy classification score'),), print_results=True):\n",
    "    \n",
    "    # make predictions\n",
    "    y_train_predicted = clf.predict(train_data[0])\n",
    "    y_test_predicted = clf.predict(test_data[0])\n",
    "    \n",
    "    metric_results = []\n",
    "    for Imetric in range(np.shape(metrics)[0]):\n",
    "        metric = metrics[Imetric][0] # select metric used\n",
    "        metric_results.append([metric(train_data[1], y_train_predicted), metric(test_data[1], y_test_predicted)])\n",
    "        \n",
    "        if print_results:\n",
    "            print(\"Train\", metrics[Imetric][1],\"for ML model:\", metric_results[-1][0])\n",
    "            print(\"Test\", metrics[Imetric][1],\"for ML model:\", metric_results[-1][1])\n",
    "        \n",
    "    return metric_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "evaluate_model(clf, (X_train_scaled, Y_train), (X_test_scaled, Y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_clf_plot(scaler, clf, variables, x3, labels, feature_labels=('Not Coilable','Coilable', 'Coilable (but yields)')):\n",
    "\n",
    "    column_names = [col for col in points.columns]\n",
    "\n",
    "    indices =  [column_names.index(name) for name in variables]\n",
    "    bounds = [data['doe_variables'][name] for name in variables]\n",
    "\n",
    "    xx = np.meshgrid(np.arange(bounds[0][0], bounds[0][1], 0.005),\n",
    "                          np.arange(bounds[1][0], bounds[1][1], 0.005))\n",
    "\n",
    "    n = len(xx[0].ravel())\n",
    "    xx.append(x3 * np.ones(n))\n",
    "\n",
    "    XX = np.empty((n, 3))\n",
    "    for index, xx_ in zip(indices, xx):\n",
    "        XX[:, index] = xx_.ravel()\n",
    "\n",
    "    XX_scaled = scaler.transform(XX)\n",
    "\n",
    "    Y = clf.predict(XX_scaled)\n",
    "    Y = Y.reshape(xx[0].shape)\n",
    "    \n",
    "    n_features = len(feature_labels)\n",
    "    colors_ = colors[:n_features]\n",
    "    custom_cm = ListedColormap(colors_)\n",
    "    \n",
    "    # plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.contourf(xx[0], xx[1], Y, cmap=custom_cm, alpha=0.5)\n",
    "    ax.set_xlabel(labels[0])\n",
    "    ax.set_ylabel(labels[1])\n",
    "    ax.set_title('%s = %.3g' % (labels[2], x3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variables = ['ratio_pitch', 'ratio_d', 'ratio_top_diameter']\n",
    "plot_labels = ['$\\\\frac{P}{D}$', '$\\\\frac{d}{D_1}$', '$\\\\frac{D_1 - D_2}{D_1}$']\n",
    "for x3 in [0., 0.25, 0.5, 0.75]:\n",
    "    make_clf_plot(scaler, clf, plot_variables, x3, plot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv3",
   "language": "python",
   "name": "mlenv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
